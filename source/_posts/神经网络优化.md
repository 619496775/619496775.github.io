---
title: ç¥ç»ç½‘ç»œä¼˜åŒ–
date: 2019-11-08 21:17:29
tags:
---
## 1

![](1.png)
![](2.png)

```python
#coding:utf-8
#é¢„æµ‹å¤šæˆ–é¢„æµ‹å°‘çš„å½±å“ä¸€æ ·
#0å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ•°æ®é›†
import tensorflow as tf
import numpy as np
BATCH_SIZE = 8
SEED = 23455

rdm = np.random.RandomState(SEED)
X = rdm.rand(32,2)
Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]

#1å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚
x = tf.placeholder(tf.float32,shape=(None,2))
y_ = tf.placeholder(tf.float32,shape=(None,1))
w1 = tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))
y = tf.matmul(x,w1)

#2å®šä¹‰æŸå¤±å‡½æ•°åŠåå‘ä¼ æ’­æ–¹æ³•ã€‚
#å®šä¹‰æŸå¤±å‡½æ•°ä¸ºMSEï¼Œåå‘ä¼ æ’­æ–¹æ³•ä¸ºæ¢¯åº¦ä¸‹é™ã€‚
loss_mse = tf.reduce_mean(tf.square(y_-y))
train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)

#ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒSTEPSè½®
with tf.Session() as sess:
init_op = tf.global_variables_initializer()
sess.run(init_op)
STEPS = 20000
for i in range(STEPS):
start = (i*BATCH_SIZE)%32
end = (i*BATCH_SIZE)%32 + BATCH_SIZE
sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})
if i %500 == 0:
print "After %d training steps, w1 is"%(i)
print  sess.run(w1)
print  "Final w1 is:\n",sess.run(w1)
#åœ¨æœ¬ä»£ç #2ä¸­å°è¯•å…¶ä»–åå‘ä¼ æ’­æ–¹æ³•ï¼Œçœ‹å¯¹æ”¶æ•›é€Ÿåº¦çš„å½±å“ï¼ŒæŠŠä½“ä¼šå†™åˆ°ç¬”è®°ä¸­
```
è¿è¡Œç»“æœå¦‚ä¸‹:

![](3.png)

ç”±ä¸Šè¿°ä»£ç å¯çŸ¥ï¼Œæœ¬ä¾‹ä¸­ç¥ç»ç½‘ç»œé¢„æµ‹æ¨¡å‹ä¸º y = w1*x1 + w2*x2ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·®ã€‚é€šè¿‡ä½¿ æŸå¤±å‡½æ•°å€¼(loss)ä¸æ–­é™ä½ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹å¾—åˆ°æœ€ç»ˆå‚æ•° w1=0.98ï¼Œw2=1.02ï¼Œé”€é‡é¢„æµ‹ç»“æœä¸º y = 0.98*x1 + 1.02*x2ã€‚ç”±äºåœ¨ç”Ÿæˆæ•°æ®é›†æ—¶ï¼Œæ ‡å‡†ç­”æ¡ˆä¸º y = x1 + x2ï¼Œå› æ­¤ï¼Œé”€é‡é¢„æµ‹ç»“æœå’Œæ ‡å‡† ç­”æ¡ˆå·²éå¸¸æ¥è¿‘ï¼Œè¯´æ˜è¯¥ç¥ç»ç½‘ç»œé¢„æµ‹é…¸å¥¶æ—¥é”€é‡æ­£ç¡®ã€‚
**è‡ªå®šä¹‰æŸå¤±å‡½æ•°:æ ¹æ®é—®é¢˜çš„å®é™…æƒ…å†µï¼Œå®šåˆ¶åˆç†çš„æŸå¤±å‡½æ•°ã€‚**

ä¾‹å¦‚: å¯¹äºé¢„æµ‹é…¸å¥¶æ—¥é”€é‡é—®é¢˜ï¼Œå¦‚æœé¢„æµ‹é”€é‡å¤§äºå®é™…é”€é‡åˆ™ä¼šæŸå¤±æˆæœ¬;å¦‚æœé¢„æµ‹é”€é‡å°äºå®é™…é”€é‡åˆ™ ä¼šæŸå¤±åˆ©æ¶¦ã€‚åœ¨å®é™…ç”Ÿæ´»ä¸­ï¼Œå¾€å¾€åˆ¶é€ ä¸€ç›’é…¸å¥¶çš„æˆæœ¬å’Œé”€å”®ä¸€ç›’é…¸å¥¶çš„åˆ©æ¶¦æ˜¯ä¸ç­‰ä»·çš„ã€‚å› æ­¤ï¼Œéœ€ è¦ä½¿ç”¨ç¬¦åˆè¯¥é—®é¢˜çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚
è‡ªå®šä¹‰æŸå¤±å‡½æ•°ä¸º:loss = âˆ‘ğ‘›ğ‘“(y_, y)
å…¶ä¸­ï¼ŒæŸå¤±å®šä¹‰æˆåˆ†æ®µå‡½æ•°:
```
f(y_,y)=ğ‘ƒğ‘…ğ‘‚ğ¹ğ¼ğ‘‡âˆ—(ğ‘¦_âˆ’ğ‘¦) ğ‘¦<ğ‘¦_ 
ğ¶ğ‘‚ğ‘†ğ‘‡âˆ—(ğ‘¦âˆ’ğ‘¦_)  ğ‘¦>=ğ‘¦_
```
æŸå¤±å‡½æ•°è¡¨ç¤ºï¼Œè‹¥é¢„æµ‹ç»“æœ y å°äºæ ‡å‡†ç­”æ¡ˆ y_ï¼ŒæŸå¤±å‡½æ•°ä¸ºåˆ©æ¶¦ä¹˜ä»¥é¢„æµ‹ç»“æœ y ä¸æ ‡å‡†ç­”æ¡ˆ y_ä¹‹å·®; è‹¥é¢„æµ‹ç»“æœ y å¤§äºæ ‡å‡†ç­”æ¡ˆ y_ï¼ŒæŸå¤±å‡½æ•°ä¸ºæˆæœ¬ä¹˜ä»¥é¢„æµ‹ç»“æœ y ä¸æ ‡å‡†ç­”æ¡ˆ y_ä¹‹å·®ã€‚
ç”¨ Tensorflow å‡½æ•°è¡¨ç¤ºä¸º:
loss = tf.reduce_sum(tf.where(tf.greater(y,y_),COST(y-y_),PROFIT(y_-y)))
1 è‹¥é…¸å¥¶æˆæœ¬ä¸º 1 å…ƒï¼Œé…¸å¥¶é”€å”®åˆ©æ¶¦ä¸º 9 å…ƒï¼Œåˆ™åˆ¶é€ æˆæœ¬å°äºé…¸å¥¶åˆ©æ¶¦ï¼Œå› æ­¤å¸Œæœ›é¢„æµ‹çš„ç»“æœ y å¤š
ä¸€äº›ã€‚é‡‡ç”¨ä¸Šè¿°çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
ä»£ç å¦‚ä¸‹:
```python
#coding:utf-8
#æˆæœ¬9å…ƒï¼Œåˆ©æ¶¦1å…ƒ
#é¢„æµ‹å¤šæˆ–é¢„æµ‹å°‘çš„å½±å“ä¸€æ ·
#0å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ•°æ®é›†
import tensorflow as tf
import numpy as np
BATCH_SIZE = 8
SEED = 23455
COST = 9
PROFIT = 1

rdm = np.random.RandomState(SEED)
X = rdm.rand(32,2)
Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]

#1å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚
x = tf.placeholder(tf.float32,shape=(None,2))
y_ = tf.placeholder(tf.float32,shape=(None,1))
w1 = tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))
y = tf.matmul(x,w1)

#2å®šä¹‰æŸå¤±å‡½æ•°åŠåå‘ä¼ æ’­æ–¹æ³•ã€‚
#å®šä¹‰æŸå¤±å‡½æ•°ä½¿å¾—é¢„æµ‹å°‘äº†çš„æŸå¤±å¤§ï¼Œäºæ˜¯åº”è¯¥å‘ä¾¿å¤šçš„æ–¹å‘é¢„æµ‹ã€‚åå‘ä¼ æ’­æ–¹æ³•ä¸ºæ¢¯åº¦ä¸‹é™ã€‚
loss_mse = tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*COST,(y_-y)*PROFIT))
train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)

#ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒSTEPSè½®
with tf.Session() as sess:
init_op = tf.global_variables_initializer()
sess.run(init_op)
STEPS = 20000
for i in range(STEPS):
start = (i*BATCH_SIZE)%32
end = (i*BATCH_SIZE)%32 + BATCH_SIZE
sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})
if i %500 == 0:
print "After %d training steps, w1 is"%(i)
print  sess.run(w1)
print  "Final w1 is:\n",sess.run(w1)
#åœ¨æœ¬ä»£ç #2ä¸­å°è¯•å…¶ä»–åå‘ä¼ æ’­æ–¹æ³•ï¼Œçœ‹å¯¹æ”¶æ•›é€Ÿåº¦çš„å½±å“ï¼ŒæŠŠä½“ä¼šå†™åˆ°ç¬”è®°ä¸­

```
è¿è¡Œç»“æœå¦‚ä¸‹:

![](4.png)

**äº¤å‰ç†µ(Cross Entropy):è¡¨ç¤ºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚äº¤å‰ç†µè¶Šå¤§ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿œï¼Œä¸¤ ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸å¼‚;äº¤å‰ç†µè¶Šå°ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿‘ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸ä¼¼ã€‚ äº¤å‰ç†µè®¡ç®—å…¬å¼:ğ‡(ğ²_ , ğ²) = âˆ’âˆ‘ğ²_ âˆ— ğ’ğ’ğ’ˆ ğ’š
ç”¨ Tensorflow å‡½æ•°è¡¨ç¤ºä¸º**
ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))

![](4.png)

## 2
**å­¦ä¹ ç‡ learning_rate:è¡¨ç¤ºäº†æ¯æ¬¡å‚æ•°æ›´æ–°çš„å¹…åº¦å¤§å°ã€‚å­¦ä¹ ç‡è¿‡å¤§ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°åœ¨æœ€ å°å€¼é™„è¿‘æ³¢åŠ¨ï¼Œä¸æ”¶æ•›;å­¦ä¹ ç‡è¿‡å°ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°æ”¶æ•›ç¼“æ…¢ã€‚ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‚æ•°çš„æ›´æ–°å‘ç€æŸå¤±å‡½æ•°æ¢¯åº¦ä¸‹é™çš„æ–¹å‘ã€‚
å‚æ•°çš„æ›´æ–°å…¬å¼ä¸º:
ğ’˜ğ’+ğŸ = ğ’˜ğ’ âˆ’ ğ’ğ’†ğ’‚ğ’“ğ’ğ’Šğ’ğ’ˆ_ğ’“ğ’‚ğ’•ğ’†ğ›**

```python
#coding:utf-8
#è®¾æŸå¤±å‡½æ•° loss=(w+1)^2,ä»¤wåˆå€¼æ˜¯å¸¸æ•°5ã€‚åå‘ä¼ æ’­å°±æ˜¯æ±‚æœ€ä¼˜wï¼Œå³æ±‚æœ€å°losså¯¹åº”çš„wå€¼
import tensorflow as tf
#å®šä¹‰å¾…ä¼˜åŒ–å‚æ•°wåˆå€¼5
w = tf.Variable(tf.constant(5,dtype=tf.float32))
#å®šä¹‰æŸå¤±å‡½æ•°loss
loss = tf.square(w+1)
#å®šä¹‰åå‘ä¼ æ’­æ–¹æ³•
train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
#ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒ40è½®
with tf.Session() as sess:
init_op = tf.global_variables_initializer()
sess.run(init_op)
for i in range(40):
sess.run(train_step)
w_val = sess.run(w)
loss_val = sess.run(loss)
print "After %s steps: w is %f.   loss is %f" % (i, w_val,loss_val)
```
è¿è¡Œç»“æœå¦‚ä¸‹:
![](5.png)
ç”±ç»“æœå¯çŸ¥ï¼Œéšç€æŸå¤±å‡½æ•°å€¼çš„å‡å°ï¼Œw æ— é™è¶‹è¿‘äº-1ï¼Œæ¨¡å‹è®¡ç®—æ¨æµ‹å‡ºæœ€ä¼˜å‚æ•° w = -1ã€‚
**å­¦ä¹ ç‡çš„è®¾ç½® å­¦ä¹ ç‡è¿‡å¤§ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°åœ¨æœ€å°å€¼é™„è¿‘æ³¢åŠ¨ï¼Œä¸æ”¶æ•›;å­¦ä¹ ç‡è¿‡å°ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°æ”¶ æ•›ç¼“æ…¢ã€‚**

**æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡:å­¦ä¹ ç‡éšç€è®­ç»ƒè½®æ•°å˜åŒ–è€ŒåŠ¨æ€æ›´æ–°**
```python
#coding:utf-8
#è®¾æŸå¤±å‡½æ•° loss=(w+1)^2,ä»¤wåˆå€¼æ˜¯å¸¸æ•°10ã€‚åå‘ä¼ æ’­å°±æ˜¯æ±‚æœ€ä¼˜wï¼Œå³æ±‚æœ€å°losså¯¹åº”çš„wå€¼
#ä½¿ç”¨æŒ‡æ•°è¡°å‡çš„å­¦ä¹ ç‡ï¼Œåœ¨è¿­ä»£åˆæœŸå¾—åˆ°è¾ƒé«˜çš„ä¸‹é™é€Ÿåº¦ï¼Œå¯ä»¥åœ¨è¾ƒå°çš„è®­ç»ƒè½®æ•°ä¸‹å–çš„æ›´ä¼˜æ”¶æ•›åº¦
import tensorflow as tf

LEARNING_RATE_BASE = 0.1 #æœ€åˆå­¦ä¹ ç‡
LEARNING_RATE_DECAY = 0.99 #å­¦ä¹ ç‡è¡°å‡ç‡
LEARNING_RATE_STEP = 1 #å–‚å…¥å¤šå°‘è½®BATCH_SIZEåï¼Œæ›´æ–°ä¸€æ¬¡å­¦ä¹ ç‡ï¼Œä¸€èˆ¬è®¾ä¸ºï¼šæ€»æ ·æœ¬æ•°/BATCH_SIZE

#è¿è¡Œäº†å‡ è½®BATCH_SIZEçš„è®¡æ•°å™¨ï¼Œåˆå€¼ç»™0ï¼Œè®¾ä¸ºä¸è¢«è®­ç»ƒ
global_step = tf.Variable(0,trainable=False)
#å®šä¹‰æŒ‡æ•°ä¸‹é™å­¦ä¹ ç‡
learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,
staircase=True)
#å®šä¹‰å¾…ä¼˜åŒ–å‚æ•°ï¼Œåˆå€¼ç»™10
w = tf.Variable(tf.constant(5,dtype=tf.float32))
#å®šä¹‰æŸå¤±å‡½æ•°loss
loss = tf.square(w+1)
#å®šä¹‰åå‘ä¼ æ’­æ–¹æ³•
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)
#ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒ40è½®
with tf.Session() as sess:
init_op = tf.global_variables_initializer()
sess.run(init_op)
for i in range(40):
sess.run(train_step)
learning_rate_val = sess.run(learning_rate)
global_step_val = sess.run(global_step)
w_val = sess.run(w)
loss_val = sess.run(loss)
print "After %s steps: global_step is %f , learning rate is %f, w is %f.   loss is %f" % (i,global_step_val,learning_rate_val, w_val,loss_val)
```
è¿è¡Œç»“æœå¦‚ä¸‹:
![](6.png)
ç”±ç»“æœå¯ä»¥çœ‹å‡ºï¼Œéšç€è®­ç»ƒè½®æ•°å¢åŠ å­¦ä¹ ç‡åœ¨ä¸æ–­å‡å°ã€‚

## 3
æ»‘åŠ¨å¹³å‡:è®°å½•äº†ä¸€æ®µæ—¶é—´å†…æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•° w å’Œ b å„è‡ªçš„å¹³å‡å€¼ã€‚åˆ©ç”¨æ»‘åŠ¨å¹³å‡å€¼å¯ä»¥å¢å¼ºæ¨¡ å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
æ»‘åŠ¨å¹³å‡å€¼(å½±å­)è®¡ç®—å…¬å¼:
å½±å­ = è¡°å‡ç‡ * å½±å­ +(1 - è¡°å‡ç‡)* å‚æ•°
å…¶ä¸­ï¼Œè¡°å‡ç‡ = ğ¦ğ¢ğ§ {ğ‘´ğ‘¶ğ‘½ğ‘°ğ‘µğ‘®ğ‘¨ğ‘½ğ‘¬ğ‘¹ğ‘¨ğ‘®ğ‘¬ğ‘«ğ‘¬ğ‘ªğ‘¨ğ’€ , ğŸ+è½®æ•° /10+è½®æ•°}ï¼Œå½±å­åˆå€¼=å‚æ•°åˆå€¼
âˆšç”¨ Tesnsorflow å‡½æ•°è¡¨ç¤ºä¸º:
âˆšema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAYï¼Œglobal_step) å…¶ä¸­ï¼ŒMOVING_AVERAGE_DECAY è¡¨ç¤ºæ»‘åŠ¨å¹³å‡è¡°å‡ç‡ï¼Œä¸€èˆ¬ä¼šèµ‹æ¥è¿‘ 1 çš„å€¼ï¼Œglobal_step è¡¨ç¤ºå½“å‰ è®­ç»ƒäº†å¤šå°‘è½®ã€‚
âˆšema_op = ema.apply(tf.trainable_variables()) å…¶ä¸­ï¼Œema.apply()å‡½æ•°å®ç°å¯¹æ‹¬å·å†…å‚æ•°æ±‚æ»‘åŠ¨å¹³å‡ï¼Œtf.trainable_variables()å‡½æ•°å®ç°æŠŠæ‰€æœ‰ å¾…è®­ç»ƒå‚æ•°æ±‡æ€»ä¸ºåˆ—è¡¨ã€‚
âˆšwith tf.control_dependencies([train_step, ema_op]):
train_op = tf.no_op(name='train')
å…¶ä¸­ï¼Œè¯¥å‡½æ•°å®ç°å°†æ»‘åŠ¨å¹³å‡å’Œè®­ç»ƒè¿‡ç¨‹åŒæ­¥è¿è¡Œã€‚
æŸ¥çœ‹æ¨¡å‹ä¸­å‚æ•°çš„å¹³å‡å€¼ï¼Œå¯ä»¥ç”¨ ema.average()å‡½æ•°ã€‚
ä¾‹å¦‚:
åœ¨ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œå°† MOVING_AVERAGE_DECAY è®¾ç½®ä¸º 0.99ï¼Œå‚æ•° w1 è®¾ç½®ä¸º 0ï¼Œw1 çš„æ»‘åŠ¨å¹³å‡å€¼è®¾ ç½®ä¸º 0ã€‚
1å¼€å§‹æ—¶ï¼Œè½®æ•° global_step è®¾ç½®ä¸º 0ï¼Œå‚æ•° w1 æ›´æ–°ä¸º 1ï¼Œåˆ™ w1 çš„æ»‘åŠ¨å¹³å‡å€¼ä¸º:
w1 æ»‘åŠ¨å¹³å‡å€¼=min(0.99,1/10)*0+(1â€“ min(0.99,1/10)*1 = 0.9
3 å½“è½®æ•° global_step è®¾ç½®ä¸º 100 æ—¶ï¼Œå‚æ•° w1 æ›´æ–°ä¸º 10ï¼Œä»¥ä¸‹ä»£ç  global_step ä¿æŒä¸º 100ï¼Œæ¯
æ¬¡æ‰§è¡Œæ»‘åŠ¨å¹³å‡æ“ä½œå½±å­å€¼æ›´æ–°ï¼Œåˆ™æ»‘åŠ¨å¹³å‡å€¼å˜ä¸º:
w1 æ»‘åŠ¨å¹³å‡å€¼=min(0.99,101/110)*0.9+(1â€“ min(0.99,101/110)*10 = 0.826+0.818=1.644 3å†æ¬¡è¿è¡Œï¼Œå‚æ•° w1 æ›´æ–°ä¸º 1.644ï¼Œåˆ™æ»‘åŠ¨å¹³å‡å€¼å˜ä¸º:
w1 æ»‘åŠ¨å¹³å‡å€¼=min(0.99,101/110)*1.644+(1â€“ min(0.99,101/110)*10 = 2.328 4å†æ¬¡è¿è¡Œï¼Œå‚æ•° w1 æ›´æ–°ä¸º 2.328ï¼Œåˆ™æ»‘åŠ¨å¹³å‡å€¼:
w1 æ»‘åŠ¨å¹³å‡å€¼=2.956
ä»£ç å¦‚ä¸‹:
```python
#coding:utf-8
import tensorflow as tf

#1. å®šä¹‰å˜é‡åŠæ»‘åŠ¨å¹³å‡ç±»
#å®šä¹‰ä¸€ä¸ª32ä½æµ®ç‚¹å˜é‡ï¼Œåˆå§‹å€¼ä½0.0  è¿™ä¸ªä»£ç å°±æ˜¯ä¸æ–­æ›´æ–°w1å‚æ•°ï¼Œä¼˜åŒ–w1å‚æ•°æ»‘åŠ¨å¹³å‡åšäº†ä¸€ä¸ªw1çš„å½±å­
w1 = tf.Variable(0,dtype=tf.float32)
#å®šä¹‰num_updates(NNçš„è¿­ä»£è½®æ•°)ï¼Œåˆå§‹å€¼ä½0ï¼Œä¸å¯è¢«ä¼˜åŒ–ï¼ˆè®­ç»ƒï¼‰ï¼Œè¿™ä¸ªå‚æ•°ä¸è®­ç»ƒ
global_step = tf.Variable(0,trainable=False)
#å®ä¾‹åŒ–æ»‘åŠ¨å¹³å‡ç±»ï¼Œç»™åˆ å‡ç‡ä¸º0ã€‚99ï¼Œå½“å‰è½®æ•°global_step
MOVING_AVERAGE_DECAY = 0.99
ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)
#ema.applyåçš„æ‹¬å·é‡Œæ˜¯æ›´æ–°åˆ—è¡¨ï¼Œæ¯æ¬¡è¿è¡Œsess.run(ema_op)æ—¶ï¼Œå¯¹æ›´æ–°åˆ—è¡¨ä¸­çš„å…ƒç´ æ±‚æ»‘åŠ¨å¹³å‡å€¼ã€‚
#åœ¨å®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨tf.trainble_variable()è‡ªåŠ¨å°†æ‰€æœ‰å¾…è®­ç»ƒçš„å‚æ•°æ±‡æ€»ä¸ºåˆ—è¡¨
#ema_op = em.apply([w1])
ema_op = ema.apply(tf.trainable_variables())

#2. æŸ¥çœ‹ä¸åŒè¿­ä»£ä¸­å˜é‡å–å€¼çš„å˜åŒ–ã€‚
with tf.Session() as sess:
#åˆå§‹åŒ–
init_op = tf.global_variables_initializer()
sess.run(init_op)
#ç”¨ema.average(w1)è·å–w1æ»‘åŠ¨å¹³å‡å€¼  ï¼ˆè¦è¿è¡Œå¤šä¸ªèŠ‚ç‚¹ï¼Œä½œä¸ºåˆ—è¡¨ä¸­çš„å…ƒç´ åˆ—å‡ºï¼Œå†™åœ¨sess.runä¸­ï¼‰
print sess.run([w1,ema.average(w1)])

# å‚æ•°w1çš„å€¼èµ‹ä¸º1
sess.run(tf.assign(w1, 1))
sess.run(ema_op)
print sess.run([w1,ema.average(w1)])

#æ›´æ–°stepå’Œw1çš„å€¼ï¼Œæ¨¡æ‹Ÿå‡º100è½®è¿­ä»£åï¼Œå‚æ•°w1å˜åŒ–ä¸º10
sess.run(tf.assign(global_step,100))
sess.run(tf.assign(w1,10))
sess.run(ema_op)
print sess.run([w1,ema.average(w1)])

# æ¯æ¬¡sess.runä¼šæ›´æ–°ä¸€æ¬¡w1çš„æ»‘åŠ¨å¹³å‡å€¼
sess.run(ema_op)
print sess.run([w1, ema.average(w1)])

sess.run(ema_op)
print sess.run([w1, ema.average(w1)])

sess.run(ema_op)
print sess.run([w1,ema.average(w1)])

sess.run(ema_op)
print sess.run([w1, ema.average(w1)])

sess.run(ema_op)
print sess.run([w1, ema.average(w1)])

sess.run(ema_op)
print sess.run([w1, ema.average(w1)])

#æ›´æ”¹MOVING_AVERAGE_DECAY ä¸º 0.1 çœ‹å½±å­è¿½éšé€Ÿåº¦

```
è¿è¡Œç¨‹åºï¼Œç»“æœå¦‚ä¸‹:
![](7.png)

ä»è¿è¡Œç»“æœå¯çŸ¥ï¼Œæœ€åˆå‚æ•° w1 å’Œæ»‘åŠ¨å¹³å‡å€¼éƒ½æ˜¯ 0;å‚æ•° w1 è®¾å®šä¸º 1 åï¼Œæ»‘åŠ¨å¹³å‡å€¼å˜ä¸º 0.9; å½“è¿­ä»£è½®æ•°æ›´æ–°ä¸º 100 è½®æ—¶ï¼Œå‚æ•° w1 æ›´æ–°ä¸º 10 åï¼Œæ»‘åŠ¨å¹³å‡å€¼å˜ä¸º 1.644ã€‚éšåæ¯æ‰§è¡Œä¸€æ¬¡ï¼Œå‚æ•° w1 çš„æ»‘åŠ¨å¹³å‡å€¼éƒ½å‘å‚æ•° w1 é è¿‘ã€‚å¯è§ï¼Œæ»‘åŠ¨å¹³å‡è¿½éšå‚æ•°çš„å˜åŒ–è€Œå˜åŒ–ã€‚

## 4
âˆšè¿‡æ‹Ÿåˆ:ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œåœ¨æ–°çš„æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–åˆ†ç±»æ—¶å‡†ç¡®ç‡è¾ƒ ä½ï¼Œè¯´æ˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å·®ã€‚
âˆšæ­£åˆ™åŒ–:åœ¨æŸå¤±å‡½æ•°ä¸­ç»™æ¯ä¸ªå‚æ•° w åŠ ä¸Šæƒé‡ï¼Œå¼•å…¥æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡ï¼Œä»è€ŒæŠ‘åˆ¶æ¨¡å‹å™ªå£°ï¼Œå‡å° è¿‡æ‹Ÿåˆã€‚
ä½¿ç”¨æ­£åˆ™åŒ–åï¼ŒæŸå¤±å‡½æ•° loss å˜ä¸ºä¸¤é¡¹ä¹‹å’Œ:
`loss = loss(y ä¸ y_) + REGULARIZER*loss(w) `å…¶ä¸­ï¼Œç¬¬ä¸€é¡¹æ˜¯é¢„æµ‹ç»“æœä¸æ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„å·®è·ï¼Œå¦‚ä¹‹å‰è®²è¿‡çš„äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ç­‰;ç¬¬äºŒé¡¹æ˜¯æ­£åˆ™
åŒ–è®¡ç®—ç»“æœã€‚
âˆšæ­£åˆ™åŒ–è®¡ç®—æ–¹æ³•:
1 L1 æ­£åˆ™åŒ–: ğ’ğ’ğ’”ğ’”ğ‘³ğŸ = âˆ‘ğ’Š|ğ’˜ğ’Š|
ç”¨ Tesnsorflow å‡½æ•°è¡¨ç¤º:loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w) 2 L2 æ­£åˆ™åŒ–: ğ’ğ’ğ’”ğ’”ğ‘³ğŸ = âˆ‘ğ’Š|ğ’˜ğ’Š|ğŸ
ç”¨ Tesnsorflow å‡½æ•°è¡¨ç¤º:loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w) âˆšç”¨ Tesnsorflow å‡½æ•°å®ç°æ­£åˆ™åŒ–:
tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w) loss = cem + tf.add_n(tf.get_collection('losses'))

âˆšmatplotlib æ¨¡å—:Python ä¸­çš„å¯è§†åŒ–å·¥å…·æ¨¡å—ï¼Œå®ç°å‡½æ•°å¯è§†åŒ– ç»ˆç«¯å®‰è£…æŒ‡ä»¤:sudo pip install matplotlib
âˆšå‡½æ•° plt.scatter():åˆ©ç”¨æŒ‡å®šé¢œè‰²å®ç°ç‚¹(x,y)çš„å¯è§†åŒ– plt.scatter (x åæ ‡, y åæ ‡, c=â€é¢œè‰²â€)
plt.show()
âˆšæ”¶é›†è§„å®šåŒºåŸŸå†…æ‰€æœ‰çš„ç½‘æ ¼åæ ‡ç‚¹:
xx, yy = np.mgrid[èµ·:æ­¢:æ­¥é•¿, èµ·:æ­¢:æ­¥é•¿] #æ‰¾åˆ°è§„å®šåŒºåŸŸä»¥æ­¥é•¿ä¸ºåˆ†è¾¨ç‡çš„è¡Œåˆ—ç½‘æ ¼åæ ‡ç‚¹ grid = np.c_[xx.ravel(), yy.ravel()] #æ”¶é›†è§„å®šåŒºåŸŸå†…æ‰€æœ‰çš„ç½‘æ ¼åæ ‡ç‚¹ âˆšplt.contour()å‡½æ•°:å‘ŠçŸ¥ xã€y åæ ‡å’Œå„ç‚¹é«˜åº¦ï¼Œç”¨ levels æŒ‡å®šé«˜åº¦çš„ç‚¹æä¸Šé¢œè‰² plt.contour (x è½´åæ ‡å€¼, y è½´åæ ‡å€¼, è¯¥ç‚¹çš„é«˜åº¦, levels=[ç­‰é«˜çº¿çš„é«˜åº¦])
plt.show()

ä»£ç å¦‚ä¸‹ï¼š
```python
# coding:utf-8
# 0 å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†
import tensorflow as tf
import numpy as np

import matplotlib.pyplot as plt
BATCH_SIZE = 30
seed = 2
# åŸºäºseedäº§ç”Ÿéšæœºæ•°
rdm = np.random.RandomState(seed)
# éšæœºæ•°è¿”å›300è¡Œ2åˆ—çš„çŸ©é˜µï¼Œè¡¨ç¤º300ç»„åæ ‡ç‚¹x0,x1ï¼‰ä½œä¸ºè¾“å…¥æ•°æ®é›†
X = rdm.randn(300,2)
# ä»Xè¿™ä¸ª300è¡Œ2åˆ—çš„çŸ©é˜µä¸­å–å‡ºä¸€è¡Œï¼Œåˆ¤æ–­å¦‚æœä¸¤ä¸ªåæ ‡çš„å¹³æ–¹å’Œå°äº2ï¼Œç»™Yèµ‹å€¼1ï¼Œå…¶ä½™èµ‹å€¼0
# ä½œä¸ºè¾“å…¥æ•°æ®é›†çš„æ ‡ç­¾ï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰
Y_ = [int(x0*x0 + x1*x1 <2) for (x0,x1) in X]
# éå†Yä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œ1èµ‹å€¼'red'å…¶ä½™èµ‹å€¼'blue'ï¼Œè¿™æ ·å¯è§†åŒ–æ˜¾ç¤ºæ—¶äººå¯ä»¥ç›´è§‚åŒºåˆ†
Y_c = [['red' if y else 'blue'] for y in Y_]
# å¯¹æ•°æ®é›†Xå’Œæ ‡ç­¾Yè¿›è¡Œshapeæ•´ç†ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ ä¸º-1è¡¨ç¤ºï¼Œéšç¬¬äºŒä¸ªå‚æ•°è®¡ç®—å¾—åˆ°ï¼Œç¬¬äºŒä¸ªå…ƒç´ è¡¨ç¤ºå¤šå°‘åˆ—ï¼ŒæŠŠXæ•´ç†ä¸ºnè¡Œ2åˆ—ï¼ŒæŠŠYæ•´ç†ä¸ºnè¡Œ1åˆ—
X = np.vstack(X).reshape(-1,2)
Y_ = np.vstack(Y_).reshape(-1,1)
print X
print Y_
print Y_c
# ç”¨plt.scatterç”»å‡ºæ•°æ®é›†Xå„è¡Œä¸­ç¬¬0åˆ—å…ƒç´ å’Œç¬¬ä¸€åˆ—å…ƒç´ çš„ç‚¹å³å„è¡Œçš„ï¼ˆx0ï¼Œx1ï¼‰ï¼Œç”¨å„è¡ŒY_cå¯¹åº”çš„å€¼è¡¨ç¤ºé¢œè‰²ï¼ˆcæ˜¯colorçš„ç¼©å†™ï¼‰
plt.scatter(X[:,0],X[:,1],c=np.squeeze(Y_c))
plt.show()

#å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹
def get_weight(shape,regularizer):
w = tf.Variable(tf.random_normal(shape),dtype=tf.float32)
tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))
return w

def get_bias(shape):
b = tf.Variable(tf.constant(0.01,shape=shape))
return b

x = tf.placeholder(tf.float32,shape=(None,2))
y_ = tf.placeholder(tf.float32,shape=(None,1))

w1 = get_weight([2,11],0.01)
b1 = get_bias([11])
y1 = tf.nn.relu(tf.matmul(x,w1)+b1)

w2 = get_weight([11,1],0.01)
b2 = get_bias([1])
y = tf.matmul(y1,w2)+b2 #è¾“å‡ºå±‚ä¸è¿‡æ¿€æ´»

#å®šä¹‰æŸå¤±å‡½æ•°
loss_mse = tf.reduce_mean(tf.square(y-y_))        #å‡æ–¹è¯¯å·®çš„æŸå¤±å‡½æ•°
loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))   #å‡æ–¹è¯¯å·®çš„æŸå¤±å‡½æ•°åŠ ä¸Šæ¯ä¸€ä¸ªæ­£åˆ™åŒ–wçš„æŸå¤±

#å®šä¹‰åå‘ä¼ æ’­æ–¹æ³•ï¼šä¸å«æ­£åˆ™åŒ–
train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_mse)

with tf.Session() as sess:
init_op = tf.global_variables_initializer()
sess.run(init_op)
STEPS = 40000
for i in range(STEPS):
start = (i*BATCH_SIZE)%300
end = start+BATCH_SIZE
sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})
if i % 2000 ==0:
loss_mse_v = sess.run(loss_mse,feed_dict={x:X,y_:Y_})
print ("After %d steps, loss is %f"%(i,loss_mse_v))
# xxåœ¨ -3åˆ°3ä¹‹é—´ä»¥æ­¥é•¿ä¸º0ã€‚01ï¼Œyyåœ¨ -3åˆ°3ä¹‹é—´ä»¥æ­¥é•¿0ã€‚01ï¼Œç”ŸæˆäºŒç»´ç½‘æ ¼åæ ‡ç‚¹
xx,yy = np.mgrid[-3:3:0.01,-3:3:0.01]
#å°†xxï¼Œyyæ‹‰ç›´ï¼Œå¹¶åˆå¹¶æˆä¸€ä¸ª2åˆ—çš„çŸ©é˜µï¼Œå¾—åˆ°ä¸€ä¸ªç½‘æ ¼åæ ‡ç‚¹çš„é›†åˆ
grid = np.c_[xx.ravel(),yy.ravel()]
#å°†ç½‘æ ¼åæ ‡ç‚¹å–‚å…¥ç¥ç»ç½‘ç»œï¼Œprobsä¸ºè¾“å‡º
probs = sess.run(y,feed_dict={x:grid})
#probsçš„shapeè°ƒæ•´æˆxxçš„æ ·å­
probs = probs.reshape(xx.shape)
print "w1:\n",sess.run(w1)
print "b1:\n",sess.run(b1)
print "w2:\n",sess.run(w2)
print "b2:\n",sess.run(b2)

plt.scatter(X[:,0],X[:,1],c=np.squeeze(Y_c))
plt.contour(xx,yy,probs,levels=[0.5])
plt.show()


#å®šä¹‰åå‘ä¼ æ’­æ–¹æ³•ï¼šåŒ…å«æ­£åˆ™åŒ–
train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_total)

with tf.Session() as sess:
init_op = tf.global_variables_initializer()
sess.run(init_op)
STEPS = 40000
for i in range(STEPS):
start = (i*BATCH_SIZE) % 300
end = start+BATCH_SIZE
sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})
if i % 2000 ==0:
loss_v = sess.run(loss_total,feed_dict={x:X,y_:Y_})
print ("After %d steps, loss is: %f"%(i,loss_v))

xx,yy = np.mgrid[-3:3:0.01,-3:3:0.01]
grid = np.c_[xx.ravel(),yy.ravel()]
probs = sess.run(y,feed_dict={x:grid})
probs = probs.reshape(xx.shape)
print "w1:\n", sess.run(w1)
print "b1:\n", sess.run(b1)
print "w2:\n", sess.run(w2)
print "b2:\n", sess.run(b2)
plt.scatter(X[:,0],X[:,1],c=np.squeeze(Y_c))
plt.contour(xx,yy,probs,levels=[0.5])
plt.show()

```
æ‰§è¡Œä»£ç ï¼Œæ•ˆæœå¦‚ä¸‹:
é¦–å…ˆï¼Œæ•°æ®é›†å®ç°å¯è§†åŒ–ï¼Œx0 + x1 < 2 çš„ç‚¹æ˜¾ç¤ºçº¢è‰²ï¼Œ x0 + x1 â‰¥2 çš„ç‚¹æ˜¾ç¤ºè“è‰²ï¼Œå¦‚å›¾æ‰€ç¤º:
![](8.png)
æ¥ç€ï¼Œæ‰§è¡Œæ— æ­£åˆ™åŒ–çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒæŠŠçº¢è‰²çš„ç‚¹å’Œè“è‰²çš„ç‚¹åˆ†å¼€ï¼Œç”Ÿæˆæ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤º:
![](9.png)
æœ€åï¼Œæ‰§è¡Œæœ‰æ­£åˆ™åŒ–çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒæŠŠçº¢è‰²çš„ç‚¹å’Œè“è‰²çš„ç‚¹åˆ†å¼€ï¼Œç”Ÿæˆæ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤º:
![](10.png)
å¯¹æ¯”æ— æ­£åˆ™åŒ–ä¸æœ‰æ­£åˆ™åŒ–æ¨¡å‹çš„è®­ç»ƒç»“æœï¼Œå¯çœ‹å‡ºæœ‰æ­£åˆ™åŒ–æ¨¡å‹çš„æ‹Ÿåˆæ›²çº¿å¹³æ»‘ï¼Œæ¨¡å‹å…·æœ‰æ›´å¥½çš„æ³› åŒ–èƒ½åŠ›ã€‚

[å‚è€ƒæ–‡çŒ®ï¼šä¸­å›½å¤§å­¦moocäººå·¥æ™ºèƒ½å®è·µï¼štensorflowç¬”è®°](https://www.icourse163.org/course/PKU-1002536002)
